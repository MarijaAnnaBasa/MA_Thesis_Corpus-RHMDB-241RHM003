# -*- coding: utf-8 -*-
"""Framing_of_events .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ps2rzRiGwLJgfY4zfekJX2fYb63YZOPz
"""

!pip install pandas matplotlib seaborn nltk tqdm

import pandas as pd
from nltk.corpus import PlaintextCorpusReader
import nltk

from google.colab import drive
drive.mount('/content/drive')

base = '/content/drive/MyDrive/Scraped_Sputnik_FULL'

folders = {
    "Corpus_1": f"{base}/Corpus_1",
    "Corpus_2": f"{base}/Corpus_2",
    "Corpus_3": f"{base}/Corpus_3"
}

#framing keywords
frames = { "External threats": ["America", "counteroffensive", "nato", "western", "provocation"],
    "Military power": ["military", "victory", "offensive", "battleground", "operation"],
    "Defensive operations": ["eliminated", "defense", "border", "security", "resist"],
    "Military offensives": ["counteroffensive", "children", "unity", "fight", "freedom"],
    "Aerial warfare": ["drone", "airstrike", "missiles", "aerial", "target"],
    "Diplomatic efforts": ["talks", "ceasefire", "mediation", "negotiations", "peace"],
    "Humanitarian impact": ["casualties", "civilians", "refugees", "wounded", "destruction"],
    "Propaganda": ["lies", "fake", "manipulation", "propaganda", "enemy"],
    "National unity": ["unity", "support", "patriotism", "pride", "honor"],
    "The Baltics": ["baltics", "latvia", "lithuania", "estonia", "alliance"]}



#iterating over all files in each corpus folder, and storing thme in three lists
articles, ids, corpora = zip(*[
    (" ".join(PlaintextCorpusReader(path, '.*').words(fid)).lower(), fid, name)
    for name, path in folders.items()
    for fid in PlaintextCorpusReader(path, '.*').fileids()
])


#assigning created frames to each article
all_articles = [
    {
        "corpus": corpus,
        "article_id": fid,
        "frame": max(frames, key=lambda f: sum(article.count(k) for k in frames[f])),
        "keyword_count": max(sum(article.count(k) for k in keywords) for keywords in frames.values())
    }
    for article, fid, corpus in zip(articles, ids, corpora)
]


df_articles = pd.DataFrame(all_articles)

df_articles.to_csv("/content/drive/MyDrive/Sputnik_Framing.csv", index=False)
print("\nArticles saved.")

import matplotlib.pyplot as plt
import seaborn as sns

# ensuring all frames are included
overall = list(frames.keys())
freqeuncy = df_articles.groupby(['corpus', 'frame']).size().unstack(fill_value=0)
freqeuncy = freqeuncy.reindex(columns=overall, fill_value=0)

plt.figure(figsize=(10,6))
sns.heatmap(freqeuncy, annot=True, fmt="d", cmap="YlGnBu")
plt.xlabel("Frames")
plt.ylabel("Corpora")
plt.show()

nltk.download('punkt')


from nltk.sentiment.vader import SentimentIntensityAnalyzer

nltk.download('vader_lexicon')
sia = SentimentIntensityAnalyzer()


#looping through all files, noting the sentiment score and storing the results
sentiment_data = [
    {
        "article_id": fid,
        "corpus": name,
        "sentiment": sia.polarity_scores(" ".join(PlaintextCorpusReader(path, '.*').words(fid)))['compound']
    }
    for name, path in folders.items()
    for fid in PlaintextCorpusReader(path, '.*').fileids()
]


df_sentiment = pd.DataFrame(sentiment_data)
df_sentiment.head()

df_articles = df_articles.rename(columns={'text_id': 'article_id'})
df_combined = pd.merge(df_articles, df_sentiment, on=['article_id', 'corpus'], how='left')

df_combined['frame'] = pd.Categorical(df_combined['frame'], categories=frames.keys(), ordered=True)

df_combined.head()

plt.figure(figsize=(10,6))
sns.barplot(data=df_combined, x='frame', y='sentiment', palette='tab20c')
plt.xticks(rotation=45, ha='right')
plt.ylabel("Average Sentiment (-1=Negative, +1=Positive)")
plt.xlabel("Frames")
plt.tight_layout()
plt.show()

from nltk.text import ConcordanceIndex
import random, os


keywords = ["military", "victory", "offensive", "battleground", "operation"]

#collecting all words across corpora
all_words = [ w.lower()
    for folder in folders
    for w in PlaintextCorpusReader(os.path.join(base, folder), '.*').words()
    if w.isalpha() ]

#recording the positions of each word
ci = ConcordanceIndex(all_words)

def kwic(word, n=5, width=15):
    offsets = ci.offsets(word)
    print(f"\n'{word.upper()}':")
    for i in random.sample(offsets, min(n, len(offsets))):
        left = " ".join(all_words[max(0, i - width//2): i])
        right = " ".join(all_words[i + 1: i + width//2])
        print(f"... {left} [{word.upper()}] {right} ...")

for word in keywords:
    kwic(word)