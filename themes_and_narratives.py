# -*- coding: utf-8 -*-
"""Themes_and_narratives.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ml58LRJpoG9ZipPClfbhD0SXeeRkpeAO
"""

!pip install gensim

import gensim
from nltk.corpus import PlaintextCorpusReader
import nltk
import gensim
from gensim import corpora
import matplotlib.pyplot as plt
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')


# downloading stopwords
nltk.download('stopwords')
stopwords = nltk.corpus.stopwords.words('english')

# normalizing words
def normalize_words(corpus, stop):
    lists_of_words = []
    for fileid in corpus.fileids():
        words = corpus.words(fileid)
        words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop]
        lists_of_words.append(words)
    return lists_of_words


def topic_modeling(corpus_path, corpus_name, num_topics=1):

    #loading the corpus
    corpus = PlaintextCorpusReader(corpus_path, '.*')
    lists_of_words = normalize_words(corpus, stopwords)

    #create dictionary and bow
    dictionary = corpora.Dictionary(lists_of_words)
    dictionary.filter_extremes(no_below=2, no_above=0.5)
    corpus_bow = [dictionary.doc2bow(words) for words in lists_of_words]

    #training the LDA model
    lda = gensim.models.LdaMulticore(
        corpus=corpus_bow,
        id2word=dictionary,
        num_topics=num_topics,
        iterations=500,
        passes=20,
        workers=3,
        random_state=42
    )

    #displaying the topics
    topics_list = []
    for i in range(lda.num_topics):
        terms = [term for term, val in lda.show_topic(i, 10)]
        print(f"Topic #{i} in {corpus_name}: {', '.join(terms)}")
        for word, weight in lda.show_topic(i, 10):
             topics_list.append({
                'Corpus': corpus_name,
                'Topic': f'Topic_{i}',
                'Word': word,
                'Weight': weight
            })


    #ploting the topics
    n = 10
    for i in range(lda.num_topics):
        plt.figure(figsize=(5, 3))
        words = [word for word, weight in lda.show_topic(i, n)]
        weights = [weight for word, weight in lda.show_topic(i, n)]
        plt.barh(range(0, n), weights)
        plt.title(f'{corpus_name} — Topic #{i}')
        plt.xlabel('Weight')
        plt.yticks(range(0, n), words)
        plt.ylim(n - 0.5, -0.5)
        plt.show()

    return pd.DataFrame(topics_list)



base = '/content/drive/MyDrive/Scraped_Sputnik_FULL'
corpus_folders = {
    "Corpus_1": f"{base}/Corpus_1",
    "Corpus_2": f"{base}/Corpus_2",
    "Corpus_3": f"{base}/Corpus_3"
}

#running topic modeling for all corpora
for name, path in corpus_folders.items():
    topic_modeling(path, name, num_topics=1)

all_topics = pd.concat([topic_modeling(path, name) for name, path in corpus_folders.items()], ignore_index=True)

import seaborn as sns

plt.figure(figsize=(12, 6))
sns.barplot(
    data=all_topics,
    x='Weight',
    y='Word',
    hue='Corpus'
)
plt.xlabel('LDA Weight')
plt.ylabel('Word')
plt.legend(title='Corpus')
plt.show()

from nltk.sentiment.vader import SentimentIntensityAnalyzer

nltk.download('vader_lexicon')
sia = SentimentIntensityAnalyzer()


 #creating three paralel lists for further analysis
documents = [
    ' '.join(words)
    for name, path in corpus_folders.items()
    for words in normalize_words(PlaintextCorpusReader(path, '.*'), stopwords)
]

file_ids = [
    fileid
    for name, path in corpus_folders.items()
    for fileid in PlaintextCorpusReader(path, '.*').fileids()
]

corpora = [
    name
    for name, path in corpus_folders.items()
    for _ in PlaintextCorpusReader(path, '.*').fileids()
]

#computing the overall sentiment score
sentiments = [sia.polarity_scores(doc)['compound'] for doc in documents]

#combining the resulsts into DataFrame
df_sentiment = pd.DataFrame({
    'Corpus': corpora,
    'File': file_ids,
    'Sentiment': sentiments
})

avg = df_sentiment.groupby('Corpus')['Sentiment'].mean().reset_index()

plt.figure(figsize=(8, 5))
sns.barplot(data=avg, x='Corpus', y='Sentiment', palette='pastel')
sns.stripplot(data=df_sentiment, x='Corpus', y='Sentiment', color='blue', alpha=0.3, jitter=True)
plt.ylabel('Average sentiment')
plt.ylim(-1, 1)
plt.show()

!pip install spacy
!python -m spacy download en_core_web_sm

import spacy
nlp = spacy.load("en_core_web_sm")


from collections import Counter

all_names = []

for doc_text in documents:  # use the list of document strings
    spacy_doc = nlp(doc_text)
    # extracting entities labeled as PERSON
    names = [ent.text.lower() for ent in spacy_doc.ents if ent.label_ == 'PERSON']  # lowercased for mapping
    all_names.extend(names)

#cleaning the results by indicating which entities are the same ones
mapping = {
    "putin": "vladimir putin",
    "zelensky": "volodymyr zelensky",
    "peskov": "dmitry peskov",
    "sergei shoigu": "sergey shoigu",
    "klymenko": "oleksandr klymenko",
    "oleksandr": "oleksandr klymenko",
    "biden": "joe biden"
}

normalized_names = [mapping.get(name, name) for name in all_names]

#adding additonal words and collocations to the stopword list
name_stoplist = [
    "kiev", "mod html", "mm", "zapad", "html ukraine", "kherson", "eu",
    "krasny liman", "gmt russian", "ukraine ukraine", "kherson zaporozhye",
    "gmt kiev", "russian su", "mod gmt", "ukraine putin", "kievs",
    "novaya kakhovka", "html kiev", "zaporozhye kherson", "kakhovka hydroelectric",
    "kakhovka hpp", "html zelensky", "html", "ukraine kiev", "html ukrainian",
    "kirby", "kiev push", "kiev regime", "mm msta", "html scott", "kilometers miles", "zaporozhye", "sputnik", "battlegroup zapad",
    "battlegroup yug", "chasov yar"
]

filtered = [name for name in normalized_names if name not in name_stoplist]

# Count frequency
name_counts = Counter(filtered)

most_common_names = name_counts.most_common(10)
import pandas as pd

df_name = pd.DataFrame(most_common_names, columns=['Name', 'Frequency'])
print(df_name)

# calculating the relative frequency of entities mentioned
total_mentions = sum(name_counts.values())
df_name['Relative_Frequency'] = df_name['Frequency'] / total_mentions

df_name['Relative_Frequency_%'] = df_name['Relative_Frequency'] * 100

print(df_name)

#creating a comprehensive visualisation for the results

plt.figure(figsize=(12, 6))
ax = sns.barplot(x='Frequency', y='Name', data=df_name, palette='viridis')

#adding counts to each bar
for i, (count, name) in enumerate(zip(df_name['Frequency'], df_name['Name'])):
    ax.text(count + 0.5, i, str(count), color='black', va='center')

plt.xlabel('Frequency')
plt.ylabel('Name')
plt.show()

from collections import Counter

corpus_name_counts = {}

for corpus_name, path in corpus_folders.items():
    corpus = PlaintextCorpusReader(path, '.*')
    all_names = []

    #extracting person entities per document
    for fileid in corpus.fileids():
        text = " ".join(corpus.words(fileid))
        spacy_doc = nlp(text)
        all_names.extend(
            ent.text.lower() for ent in spacy_doc.ents if ent.label_ == 'PERSON'
        )

    normalized = [mapping.get(name, name) for name in all_names]
    filtered = [name for name in normalized if name not in name_stoplist]

    counts = Counter(filtered)
    corpus_name_counts[corpus_name] = counts

    #Preparing top 3 most common entities for the visualisation
    df_name = pd.DataFrame(counts.most_common(3), columns=['Name', 'Count'])

    print(f"\nTop names in {corpus_name}:")
    print(df_name)

    # Plot
    plt.figure(figsize=(10, 5))
    ax = sns.barplot(x='Count', y='Name', data=df_name, palette='viridis')

    # Add count labels
    for i, (count, name) in enumerate(zip(df_name['Count'], df_name['Name'])):
        ax.text(count + 0.5, i, str(count), color='black', va='center')

    plt.title(f'Top Representative Mentions — {corpus_name}')
    plt.xlabel('Frequency')
    plt.ylabel('Name')
    plt.tight_layout()
    plt.show()