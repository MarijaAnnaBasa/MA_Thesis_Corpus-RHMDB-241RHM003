# -*- coding: utf-8 -*-
"""Scraped_Sputnik.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MG6M3L6AnaKwqBnswC2TulRt4c2QN_6t
"""

!pip install requests beautifulsoup4 pandas openpyxl

import os
import requests
import pandas as pd

from bs4 import BeautifulSoup

from google.colab import drive

drive.mount('/content/drive')
path = '/content/drive/MyDrive/MA_Corpus/Sputnik/Sputnik_FULL.xlsx'

df = pd.read_excel(path)
# removing any empty cells
links = df.iloc[:, 1].dropna().tolist()

# creating the respective folder automatically
output = '/content/drive/MyDrive/Scraped_Sputnik_FULL'
os.makedirs(output, exist_ok=True)


# nd = no data
def scraped(url):
    try:
        r = requests.get(url)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, 'html.parser')

        title = soup.find('h1', class_='article__title')
        date = soup.find('div', class_='article__info-date')
        text = soup.find('div', class_='article__body')

        return (
            title.get_text(strip=True) if title else "nd",
            date.get_text(strip=True) if date else "nd",
            text.get_text('\n', strip=True) if text else "nd"
        )
    except:
        return "nd", "nd", "nd"


for i, link in enumerate(links, start=1):
    title, date, text = scraped(link)

    data = {
        'URL': [link],
        'Title': [title],
        'Date': [date],
        'Text': [text]
    }

    df_article = pd.DataFrame(data)

    # eg of file name 001
    name = os.path.join(output, f"article_{i:03}.csv")
    df_article.to_csv(name, index=False, encoding='utf-8-sig')

    print(f"[{i}/{len(links)}] Saved: {name}")


print("\n All saved")

import re
from datetime import datetime

base = '/content/drive/MyDrive/Scraped_Sputnik_FULL'

#cleaning the main corpus for three time intervals
corpora = {
    "Corpus_1": (datetime(2023,2,24), datetime(2024,2,24)),
    "Corpus_2": (datetime(2024,2,25), datetime(2025,2,24)),
    "Corpus_3": (datetime(2025,2,25), datetime(2025,7,24))
}

csv_files = [os.path.join(base, f) for f in os.listdir(base) if f.endswith('.csv')]

#converting the string
def sputnik(date):
    if pd.isna(date):
        return None
    try:
        return datetime.strptime(re.search(r'\d{2}\.\d{2}\.\d{4}', date).group(), "%d.%m.%Y")
    except:
        return None


#creating seperate folders under the main one
for name, (start_date, end_date) in corpora.items():
    output = os.path.join(base, name)
    os.makedirs(output, exist_ok=True)

#filtering of data and saving it to the right corpus
    for file in csv_files:
      try:
        df = pd.read_csv(file, encoding='utf-8')
        if 'Date' in df.columns:
            cleaned = df[df['Date'].apply(sputnik).between(start_date, end_date)]
            if not cleaned.empty:
                cleaned.to_csv(
                    os.path.join(output, os.path.basename(file)),
                    index=False, encoding='utf-8-sig'
                )



print("\n All saved")